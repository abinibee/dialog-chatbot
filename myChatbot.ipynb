{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity      \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import spacy\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>some of it is okay, i guess.</td>\n",
       "      <td>yes, the poems that rhyme and are easy to reme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>me too.</td>\n",
       "      <td>where does cheese come from?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>no, i couldn't make it.</td>\n",
       "      <td>you missed a really good game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>they still make movies like that.</td>\n",
       "      <td>yes, but they never make much money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>well, we have a new president.</td>\n",
       "      <td>but we have the same old problems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>i don't know anyone who likes it.</td>\n",
       "      <td>some of it is okay, i guess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3287</th>\n",
       "      <td>that means it gets twice as much traffic.</td>\n",
       "      <td>you're right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>those books will slip and you'll fall.</td>\n",
       "      <td>it's only a couple of feet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>where's the car?</td>\n",
       "      <td>what do you mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>thank you.</td>\n",
       "      <td>i'm so happy for you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Question  \\\n",
       "1473               some of it is okay, i guess.   \n",
       "3041                                    me too.   \n",
       "362                     no, i couldn't make it.   \n",
       "2141          they still make movies like that.   \n",
       "3403             well, we have a new president.   \n",
       "1472          i don't know anyone who likes it.   \n",
       "3287  that means it gets twice as much traffic.   \n",
       "2629     those books will slip and you'll fall.   \n",
       "1834                           where's the car?   \n",
       "221                                  thank you.   \n",
       "\n",
       "                                                 Answer  \n",
       "1473  yes, the poems that rhyme and are easy to reme...  \n",
       "3041                       where does cheese come from?  \n",
       "362                      you missed a really good game.  \n",
       "2141               yes, but they never make much money.  \n",
       "3403                 but we have the same old problems.  \n",
       "1472                       some of it is okay, i guess.  \n",
       "3287                                      you're right.  \n",
       "2629                        it's only a couple of feet.  \n",
       "1834                                  what do you mean?  \n",
       "221                               i'm so happy for you.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dialogs.txt\",  sep= '\\t', na_filter=False, header = None)\n",
    "data.rename(columns = {0: 'Question', 1: 'Answer'}, inplace = True )\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the Next step is tokenize our text dataset.<br>\n",
    "There are two types of tokenization:\n",
    "    <ol><li>Word Tokenization: This is  the process of breaking down a text or document into individual words or tokens.</li>\n",
    "    <li>Sent Tokenization: This is to break down the text data into individual sentences so that each sentence can be processed separately.</li><br></ol>\n",
    "Lemmatization: The goal of lemmatization is to reduce a word to its canonical form so that variations of the same word can be treated as the same token<br>\n",
    "For example, the word \"jumped\" may be lemmatized to \"jump\", and the word \"walking\" may be lemmatized to \"walk\".<br>\n",
    "By reducing words to their base forms, lemmatization can help to simplify text data and reduce the number of unique tokens that need to be analyzed or processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3722 entries, 0 to 3721\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Question  3722 non-null   object\n",
      " 1   Answer    3722 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 58.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>tokenized Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>hi how are you doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td></td>\n",
       "      <td>i pretty good thanks for asking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td></td>\n",
       "      <td>no problem so how have you been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>i been great what about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what school do you go to? i go to pcc.</td>\n",
       "      <td></td>\n",
       "      <td>what school do you go to i go to pcc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Question  \\\n",
       "0                  hi, how are you doing?   \n",
       "1     i'm pretty good. thanks for asking.   \n",
       "2       no problem. so how have you been?   \n",
       "3        i've been great. what about you?   \n",
       "4  what school do you go to? i go to pcc.   \n",
       "\n",
       "                                     Answer  \\\n",
       "0             i'm fine. how about yourself?   \n",
       "1                                             \n",
       "2                                             \n",
       "3  i've been good. i'm in school right now.   \n",
       "4                                             \n",
       "\n",
       "                     tokenized Question  \n",
       "0                  hi how are you doing  \n",
       "1       i pretty good thanks for asking  \n",
       "2       no problem so how have you been  \n",
       "3           i been great what about you  \n",
       "4  what school do you go to i go to pcc  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define a function for text preprocessing (including lemmatization)\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Identifies all sentences in the data\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize and lemmatize each word in each sentence\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(sentence) if word.isalnum()]\n",
    "        # Turns to basic root - each word in the tokenized word found in the tokenized sentence - if they are all alphanumeric \n",
    "        # The code above does the following:\n",
    "        # Identifies every word in the sentence \n",
    "        # Turns it to a lower case \n",
    "        # Lemmatizes it if the word is alphanumeric\n",
    "\n",
    "        preprocessed_sentence = ' '.join(tokens)\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "    \n",
    "    return ' '.join(preprocessed_sentences)\n",
    "\n",
    "\n",
    "data['tokenized Question'] = data['Question'].apply(preprocess_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus by flattening the preprocessed questions\n",
    "corpus = data['tokenized Question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize corpus\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(corpus)\n",
    "# TDIDF is a numerical statistic used to evaluate how important a word is to a document in a collection or corpus. \n",
    "# The TfidfVectorizer calculates the Tfidf values for each word in the corpus and uses them to create a matrix where each row represents a document and each column represents a word. \n",
    "# The cell values in the matrix correspond to the importance of each word in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot: i'm fine. how about yourself?\n",
      "\n",
      "Chatbot: all right, see you.\n",
      "\n",
      "Chatbot: i'm attending pcc right now.\n"
     ]
    }
   ],
   "source": [
    "def get_response(user_input):\n",
    "    global most_similar_index\n",
    "    \n",
    "    user_input_processed = preprocess_text(user_input) # ....................... Preprocess the user's input using the preprocess_text function\n",
    "\n",
    "    user_input_vector = tfidf_vectorizer.transform([user_input_processed])# .... Vectorize the preprocessed user input using the TF-IDF vectorizer\n",
    "\n",
    "    similarity_scores = cosine_similarity(user_input_vector, X) # .. Calculate the score of similarity between the user input vector and the corpus (df) vector\n",
    "\n",
    "    most_similar_index = similarity_scores.argmax() # ..... Find the index of the most similar question in the corpus (df) based on cosine similarity\n",
    "\n",
    "    return data['Answer'].iloc[most_similar_index] # ... Retrieve the corresponding answer from the df DataFrame and return it as the chatbot's response\n",
    "\n",
    "# create greeting list \n",
    "greetings = [\"Hi.... This is the voice of the guy Abinibee! .... I'm ready to help\",\n",
    "            \"Hello bros.... How you dey\",\n",
    "            'Respect!, wetin dey happen nah',\n",
    "            'How far my blood, wetin dey sup'\n",
    "            \"Good Day .... How can I help\", \n",
    "            \"Hello There... How can I be useful to you today\",\n",
    "            \"Hi Abinibee fam.... Any show for me?\"]\n",
    "\n",
    "exits = ['thanks bye', 'bye', 'quit', 'exit', 'bye bye', 'close']\n",
    "farewell = ['Thanks....see you soon', 'Babye, See you soon', 'Bye... See you later', 'Bye... come back soon']\n",
    "\n",
    "random_farewell = random.choice(farewell) # ---------------- Randomly select a farewell message from the list\n",
    "random_greetings = random.choice(greetings) # -------- Randomly select greeting message from the list\n",
    "\n",
    "# Test your chatbot\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in exits:\n",
    "        print(f\"\\nChatbot: {random_farewell}!\")\n",
    "        break\n",
    "    if user_input.lower() in ['hi', 'hello', 'hey', 'hi there']:\n",
    "        print(f\"\\nChatbot: {random_greetings}!\")\n",
    "    else:   \n",
    "        response = get_response(user_input)\n",
    "        print(f\"\\nChatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3><b>Model Technique</b></h3> <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "xtrain = tfidf_vectorizer.fit_transform(data['tokenized Question'])\n",
    "# Xtrain is the preprocessed questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform the Y \n",
    "data['Answer_ID'] = le.fit_transform(data['Answer'])\n",
    "data.head()\n",
    "\n",
    "ytrain = data['Answer_ID'].values\n",
    "# ytrain is the transformed Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(xtrain, ytrain)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtrain, ytrain)\n",
    "\n",
    "train_predict = mnb.predict(xtrain)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(train_predict, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input):\n",
    "    global results\n",
    "    user_input_processed = preprocess_text(user_input) # ....................... Preprocess the user's input using the preprocess_text function\n",
    "\n",
    "    user_input_vector = tfidf_vectorizer.transform([user_input_processed])# .... Vectorize the preprocessed user input using the TF-IDF vectorizer\n",
    "\n",
    "    results = mnb.predict(user_input_vector)\n",
    "\n",
    "    for elem in results:\n",
    "        row_df = data.loc[data.isin([elem]).any(axis=1)]\n",
    "        print(row_df['Answer'].values)\n",
    "\n",
    "# create greeting list \n",
    "greetings = [\"Hi.... This is the voice of the guy Abinibee! .... I'm ready to help\",\n",
    "            \"Hello bros.... How you dey\",\n",
    "            'Respect!, wetin dey happen nah',\n",
    "            'How far my blood, wetin dey sup'\n",
    "            \"Good Day .... How can I help\", \n",
    "            \"Hello There... How can I be useful to you today\",\n",
    "            \"Hi Abinibee fam.... Any show for me?\"]\n",
    "\n",
    "exits = ['thanks bye', 'bye', 'quit', 'later nah', 'exit', 'bye bye', 'close']\n",
    "farewell = ['Thanks....see you soon', 'Babye, See you soon', 'Bye... See you later', 'Bye... come back soon']\n",
    "\n",
    "random_farewell = random.choice(farewell) # ---------------- Randomly select a farewell message from the list\n",
    "random_greetings = random.choice(greetings) # -------- Randomly select greeting message from the list\n",
    "\n",
    "# Test your chatbot\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in exits:\n",
    "        print(f\"\\nChatbot: {random_farewell}!\")\n",
    "        break\n",
    "    if user_input.lower() in ['hi', 'hello', 'hey', 'hi there']:\n",
    "        print(f\"\\nChatbot: {random_greetings}!\")\n",
    "    else:   \n",
    "        response = get_response(user_input)\n",
    "        print(f\"\\nChatbot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
